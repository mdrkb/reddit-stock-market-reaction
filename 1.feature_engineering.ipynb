{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from collections import Counter\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"font.size\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_csv = \"data/submissions.csv\"\n",
    "comments_csv = \"data/comments_pratik.csv\"\n",
    "feature_comments_csv = \"data/feature_comments_rakib_new.csv\"\n",
    "\n",
    "date_fmt = lambda x: datetime.utcfromtimestamp(int(x)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load submissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1197819 entries, 0 to 1197818\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   author                 1197819 non-null  object        \n",
      " 1   created_utc            1197819 non-null  datetime64[ns]\n",
      " 2   id                     1197819 non-null  object        \n",
      " 3   link_flair_text        1145788 non-null  object        \n",
      " 4   num_comments           1197819 non-null  int64         \n",
      " 5   removed_by_category    897146 non-null   object        \n",
      " 6   score                  1197819 non-null  int64         \n",
      " 7   selftext               693501 non-null   object        \n",
      " 8   subreddit_subscribers  1197819 non-null  int64         \n",
      " 9   title                  1197818 non-null  object        \n",
      " 10  upvote_ratio           1084007 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(6)\n",
      "memory usage: 100.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load submissions data\n",
    "df_submissions = pd.read_csv(submissions_csv, delimiter=\";\", parse_dates=[\"created_utc\"],\n",
    "                 date_parser=date_fmt, encoding=\"utf-8\")\n",
    "df_submissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>removed_by_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1196859</th>\n",
       "      <td>mh96r3</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>crockett5</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>A poor Ape doesn't have to always be retarded,...</td>\n",
       "      <td>I'm not trying to give financial advice for du...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196860</th>\n",
       "      <td>mh96t7</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>Madmonkey45</td>\n",
       "      <td>News</td>\n",
       "      <td>Could we see another surge in pot stocks as NY...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>moderator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196861</th>\n",
       "      <td>mh96wm</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>BRENBRENBRENBREN1</td>\n",
       "      <td>News</td>\n",
       "      <td>Genius brands</td>\n",
       "      <td>Is genius brands a good buy?</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196862</th>\n",
       "      <td>mh970t</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>Leetomnsx</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>ü¶ç is getting ready to go buy a bank to deposit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196863</th>\n",
       "      <td>mh97ec</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>allintraders</td>\n",
       "      <td>YOLO</td>\n",
       "      <td>Letssss Gooooooooo!!!!!! Hertz!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id created_utc             author link_flair_text  \\\n",
       "1196859  mh96r3  2021-03-31          crockett5      Discussion   \n",
       "1196860  mh96t7  2021-03-31        Madmonkey45            News   \n",
       "1196861  mh96wm  2021-03-31  BRENBRENBRENBREN1            News   \n",
       "1196862  mh970t  2021-03-31          Leetomnsx      Discussion   \n",
       "1196863  mh97ec  2021-03-31       allintraders            YOLO   \n",
       "\n",
       "                                                     title  \\\n",
       "1196859  A poor Ape doesn't have to always be retarded,...   \n",
       "1196860  Could we see another surge in pot stocks as NY...   \n",
       "1196861                                      Genius brands   \n",
       "1196862  ü¶ç is getting ready to go buy a bank to deposit...   \n",
       "1196863                    Letssss Gooooooooo!!!!!! Hertz!   \n",
       "\n",
       "                                                  selftext  num_comments  \\\n",
       "1196859  I'm not trying to give financial advice for du...            13   \n",
       "1196860                                                NaN             2   \n",
       "1196861                       Is genius brands a good buy?             3   \n",
       "1196862                                                NaN             0   \n",
       "1196863                                                NaN             1   \n",
       "\n",
       "         score  upvote_ratio removed_by_category  \n",
       "1196859      1           1.0                 NaN  \n",
       "1196860      1           1.0           moderator  \n",
       "1196861      1           1.0                 NaN  \n",
       "1196862      1           1.0                 NaN  \n",
       "1196863      1           1.0                 NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submissions[[\"id\",\"created_utc\", \"author\",\"link_flair_text\",\"title\",\"selftext\",\"num_comments\",\"score\",\n",
    "                \"upvote_ratio\",\"removed_by_category\"]].tail(1000)[40:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6501377 entries, 0 to 6501376\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   author       object        \n",
      " 1   body         object        \n",
      " 2   created_utc  datetime64[ns]\n",
      " 3   id           object        \n",
      " 4   link_id      object        \n",
      " 5   score        int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 297.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_comments = pd.read_csv(comments_csv, delimiter=\";\", parse_dates=[\"created_utc\"],\n",
    "                date_parser=date_fmt, encoding=\"utf-8\")\n",
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>link_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g90aae4</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>0laugh</td>\n",
       "      <td>Shits gonna drop to like $78. Then load up on ...</td>\n",
       "      <td>3</td>\n",
       "      <td>t3_jcbasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g90af9u</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>DiepioHybrid</td>\n",
       "      <td>I would if I hadn't dropped my last $50 on thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_jcbasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>g90akra</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>mechanic-panic</td>\n",
       "      <td>I bought in at $95 after it crashed, I thought...</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_jcbasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>g90amr5</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>askingforafavor12345</td>\n",
       "      <td>Gonna drop to mid $70s and slowly recover over...</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_jcbasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>g90b3j0</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>0laugh</td>\n",
       "      <td>75-79 will be it's support then sky rocket again.</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_jcbasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id created_utc                author  \\\n",
       "8   g90aae4  2020-10-16                0laugh   \n",
       "9   g90af9u  2020-10-16          DiepioHybrid   \n",
       "10  g90akra  2020-10-16        mechanic-panic   \n",
       "11  g90amr5  2020-10-16  askingforafavor12345   \n",
       "12  g90b3j0  2020-10-16                0laugh   \n",
       "\n",
       "                                                 body  score    link_id  \n",
       "8   Shits gonna drop to like $78. Then load up on ...      3  t3_jcbasm  \n",
       "9   I would if I hadn't dropped my last $50 on thi...      1  t3_jcbasm  \n",
       "10  I bought in at $95 after it crashed, I thought...      2  t3_jcbasm  \n",
       "11  Gonna drop to mid $70s and slowly recover over...      2  t3_jcbasm  \n",
       "12  75-79 will be it's support then sky rocket again.      2  t3_jcbasm  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[[\"id\",\"created_utc\",\"author\",\"body\",\"score\",\"link_id\"]].head(20)[8:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Load S&P 500 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load S&P 500 tickers\n",
    "with open(\"data/sp500-symbol-list.txt\") as tickers:\n",
    "    sp500 = [ticker.rstrip() for ticker in tickers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Load non S&P 500 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non S&P 500 tickers\n",
    "nasdaq_stocks = pd.read_csv(\"data/nasdaq_stocks.csv\", sep=\",\")\n",
    "nasdaq_stocks_symbols = nasdaq_stocks[\"Symbol\"].tolist()\n",
    "non_sp500 = list(set(nasdaq_stocks_symbols) - set(sp500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Load lingos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walstreetbets lingo ref: https://economictimes.indiatimes.com/markets/stocks/news/from-stonks-to-bagholder-deciphering-reddits-trading-lingo\n",
    "lingo = [\"YOLO\", \"BAGHOLDER\", \"TENDIES\", \"DD\", \"GO\", \"BUY\", \"BUYING\", \"buying\", \"go\", \"holding\", \"high\", \n",
    "         \"moon\", \"squeeze\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Filter removed submissions\n",
    "\n",
    "Remove submissions that are deleted by author or moderator or Reddit filter.\n",
    "- **anti_evil_ops, author, deleted**: Post was removed by author.\n",
    "- **automod_filtered**: Post is awaiting moderator approval.\n",
    "- **moderator**: Post was removed by moderator.\n",
    "- **reddit**: Post was removed by Reddit's spam filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ILL3NITVM</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>eib3bh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy new year to everyone, stay hydrated &amp;amp...</td>\n",
       "      <td>772368</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theOriginalTurd</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>eib3w8</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Yesterday I bought 130 shares of ACB, and toda...</td>\n",
       "      <td>772368</td>\n",
       "      <td>Cannabis Industry Bull Run</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>winkerpack</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>eibdob</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>What better way to bring in 2020 than with a G...</td>\n",
       "      <td>772380</td>\n",
       "      <td>GUH! Your way into the New Year! Time your 202...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EagleInvestors</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>eibfcr</td>\n",
       "      <td>YOLO</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Just as the title implies.. What would you cuc...</td>\n",
       "      <td>772386</td>\n",
       "      <td>Which Stonk is primed for a YOLO?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Swipe4Swipes</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>eibn2v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>772399</td>\n",
       "      <td>My fellow stock traders ! Do you guys prefer t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author created_utc      id link_flair_text  num_comments  score  \\\n",
       "3         ILL3NITVM  2020-01-01  eib3bh             NaN             0      1   \n",
       "5   theOriginalTurd  2020-01-01  eib3w8      Discussion            18      1   \n",
       "8        winkerpack  2020-01-01  eibdob             NaN            28      1   \n",
       "9    EagleInvestors  2020-01-01  eibfcr            YOLO            17      1   \n",
       "10     Swipe4Swipes  2020-01-01  eibn2v             NaN             0      1   \n",
       "\n",
       "                                             selftext  subreddit_subscribers  \\\n",
       "3   Happy new year to everyone, stay hydrated &amp...                 772368   \n",
       "5   Yesterday I bought 130 shares of ACB, and toda...                 772368   \n",
       "8   What better way to bring in 2020 than with a G...                 772380   \n",
       "9   Just as the title implies.. What would you cuc...                 772386   \n",
       "10                                                NaN                 772399   \n",
       "\n",
       "                                                title  upvote_ratio  \n",
       "3                                     HAPPY NEW YEAR!           NaN  \n",
       "5                          Cannabis Industry Bull Run           NaN  \n",
       "8   GUH! Your way into the New Year! Time your 202...           NaN  \n",
       "9                   Which Stonk is primed for a YOLO?           NaN  \n",
       "10  My fellow stock traders ! Do you guys prefer t...           NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submissions = df_submissions[df_submissions[\"removed_by_category\"].isnull()]\n",
    "df_submissions.drop(\"removed_by_category\", axis=1, inplace=True)\n",
    "df_submissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filter removed comments\n",
    "\n",
    "Remove comments associated with removed submissions. Only keep comments from non-removed submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_comments[df_comments[\"author\"] != \"[deleted]\"]\n",
    "df_comments = df_comments[~df_comments[\"created_utc\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>I already got in but decided to say YOLO and s...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35t1p</td>\n",
       "      <td>t3_l6x2gv</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2gv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>Yeah the thing is I actually have some faith i...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl377lu</td>\n",
       "      <td>t3_l6x2gv</td>\n",
       "      <td>2</td>\n",
       "      <td>l6x2gv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sf9191</td>\n",
       "      <td>I‚Äôve been with them for two years and have hat...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl389xu</td>\n",
       "      <td>t3_l6x2hd</td>\n",
       "      <td>1</td>\n",
       "      <td>l6x2hd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amcneel</td>\n",
       "      <td>BB</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35j7i</td>\n",
       "      <td>t3_l6x2hj</td>\n",
       "      <td>6</td>\n",
       "      <td>l6x2hj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MineIsLongerThanYour</td>\n",
       "      <td>Not now man. Focus on the fucking gme</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35kwp</td>\n",
       "      <td>t3_l6x2ho</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2ho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author                                               body  \\\n",
       "1    ADONIS_VON_MEGADONG  I already got in but decided to say YOLO and s...   \n",
       "3    ADONIS_VON_MEGADONG  Yeah the thing is I actually have some faith i...   \n",
       "7                 sf9191  I‚Äôve been with them for two years and have hat...   \n",
       "9                amcneel                                                 BB   \n",
       "15  MineIsLongerThanYour              Not now man. Focus on the fucking gme   \n",
       "\n",
       "   created_utc       id    link_id  score parent_id  \n",
       "1   2021-01-28  gl35t1p  t3_l6x2gv      3    l6x2gv  \n",
       "3   2021-01-28  gl377lu  t3_l6x2gv      2    l6x2gv  \n",
       "7   2021-01-28  gl389xu  t3_l6x2hd      1    l6x2hd  \n",
       "9   2021-01-28  gl35j7i  t3_l6x2hj      6    l6x2hj  \n",
       "15  2021-01-28  gl35kwp  t3_l6x2ho      3    l6x2ho  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[\"parent_id\"] = df_comments[\"link_id\"].apply(lambda x: x.split(\"_\")[1])\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_x</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc_x</th>\n",
       "      <th>id_x</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score_x</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author_y</th>\n",
       "      <th>created_utc_y</th>\n",
       "      <th>id_y</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score_y</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>I already got in but decided to say YOLO and s...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35t1p</td>\n",
       "      <td>t3_l6x2gv</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I set an order for some additional shares of B...</td>\n",
       "      <td>4517630</td>\n",
       "      <td>Just FYI Robinhood is restricting the sale of ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>Yeah the thing is I actually have some faith i...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl377lu</td>\n",
       "      <td>t3_l6x2gv</td>\n",
       "      <td>2</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I set an order for some additional shares of B...</td>\n",
       "      <td>4517630</td>\n",
       "      <td>Just FYI Robinhood is restricting the sale of ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sf9191</td>\n",
       "      <td>I‚Äôve been with them for two years and have hat...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl389xu</td>\n",
       "      <td>t3_l6x2hd</td>\n",
       "      <td>1</td>\n",
       "      <td>l6x2hd</td>\n",
       "      <td>sf9191</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>l6x2hd</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4517657</td>\n",
       "      <td>Ally trading platform appears to allow GME tra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amcneel</td>\n",
       "      <td>BB</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35j7i</td>\n",
       "      <td>t3_l6x2hj</td>\n",
       "      <td>6</td>\n",
       "      <td>l6x2hj</td>\n",
       "      <td>auldwardog</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>l6x2hj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Can we as a group replicate what happened with...</td>\n",
       "      <td>4517657</td>\n",
       "      <td>AMC, NOK, BB - which is next??</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MineIsLongerThanYour</td>\n",
       "      <td>Not now man. Focus on the fucking gme</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35kwp</td>\n",
       "      <td>t3_l6x2ho</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2ho</td>\n",
       "      <td>Abject_Opportunity35</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>l6x2ho</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4517657</td>\n",
       "      <td>BUY AND HOLD $NOK WE HAVE TO ALL COME TOGETHERüöÄ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_x                                               body  \\\n",
       "0   ADONIS_VON_MEGADONG  I already got in but decided to say YOLO and s...   \n",
       "1   ADONIS_VON_MEGADONG  Yeah the thing is I actually have some faith i...   \n",
       "2                sf9191  I‚Äôve been with them for two years and have hat...   \n",
       "3               amcneel                                                 BB   \n",
       "4  MineIsLongerThanYour              Not now man. Focus on the fucking gme   \n",
       "\n",
       "  created_utc_x     id_x    link_id  score_x parent_id              author_y  \\\n",
       "0    2021-01-28  gl35t1p  t3_l6x2gv        3    l6x2gv   ADONIS_VON_MEGADONG   \n",
       "1    2021-01-28  gl377lu  t3_l6x2gv        2    l6x2gv   ADONIS_VON_MEGADONG   \n",
       "2    2021-01-28  gl389xu  t3_l6x2hd        1    l6x2hd                sf9191   \n",
       "3    2021-01-28  gl35j7i  t3_l6x2hj        6    l6x2hj            auldwardog   \n",
       "4    2021-01-28  gl35kwp  t3_l6x2ho        3    l6x2ho  Abject_Opportunity35   \n",
       "\n",
       "  created_utc_y    id_y link_flair_text  num_comments  score_y  \\\n",
       "0    2021-01-28  l6x2gv      Discussion             4        1   \n",
       "1    2021-01-28  l6x2gv      Discussion             4        1   \n",
       "2    2021-01-28  l6x2hd      Discussion             2        1   \n",
       "3    2021-01-28  l6x2hj             NaN             7        1   \n",
       "4    2021-01-28  l6x2ho      Discussion             1        1   \n",
       "\n",
       "                                            selftext  subreddit_subscribers  \\\n",
       "0  I set an order for some additional shares of B...                4517630   \n",
       "1  I set an order for some additional shares of B...                4517630   \n",
       "2                                                NaN                4517657   \n",
       "3  Can we as a group replicate what happened with...                4517657   \n",
       "4                                                NaN                4517657   \n",
       "\n",
       "                                               title  upvote_ratio  \n",
       "0  Just FYI Robinhood is restricting the sale of ...           1.0  \n",
       "1  Just FYI Robinhood is restricting the sale of ...           1.0  \n",
       "2  Ally trading platform appears to allow GME tra...           1.0  \n",
       "3                     AMC, NOK, BB - which is next??           1.0  \n",
       "4    BUY AND HOLD $NOK WE HAVE TO ALL COME TOGETHERüöÄ           1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do inner join which will remove the comments whose id is not in submissions dataframe\n",
    "df_comments = pd.merge(df_comments, df_submissions, left_on=\"parent_id\", right_on=\"id\", how=\"inner\")\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_x</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc_x</th>\n",
       "      <th>id_x</th>\n",
       "      <th>score_x</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>I already got in but decided to say YOLO and s...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35t1p</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2gv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>Yeah the thing is I actually have some faith i...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl377lu</td>\n",
       "      <td>2</td>\n",
       "      <td>l6x2gv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sf9191</td>\n",
       "      <td>I‚Äôve been with them for two years and have hat...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl389xu</td>\n",
       "      <td>1</td>\n",
       "      <td>l6x2hd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amcneel</td>\n",
       "      <td>BB</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35j7i</td>\n",
       "      <td>6</td>\n",
       "      <td>l6x2hj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MineIsLongerThanYour</td>\n",
       "      <td>Not now man. Focus on the fucking gme</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35kwp</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2ho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_x                                               body  \\\n",
       "0   ADONIS_VON_MEGADONG  I already got in but decided to say YOLO and s...   \n",
       "1   ADONIS_VON_MEGADONG  Yeah the thing is I actually have some faith i...   \n",
       "2                sf9191  I‚Äôve been with them for two years and have hat...   \n",
       "3               amcneel                                                 BB   \n",
       "4  MineIsLongerThanYour              Not now man. Focus on the fucking gme   \n",
       "\n",
       "  created_utc_x     id_x  score_x parent_id  \n",
       "0    2021-01-28  gl35t1p        3    l6x2gv  \n",
       "1    2021-01-28  gl377lu        2    l6x2gv  \n",
       "2    2021-01-28  gl389xu        1    l6x2hd  \n",
       "3    2021-01-28  gl35j7i        6    l6x2hj  \n",
       "4    2021-01-28  gl35kwp        3    l6x2ho  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new comments dataframe\n",
    "df_comments = df_comments[[\"author_x\", \"body\", \"created_utc_x\", \"id_x\", \"score_x\", \"parent_id\"]]\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Calculate sentiment per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'emoji']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "if \"spacytextblob\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"spacytextblob\")\n",
    "    nlp.rename_pipe(\"spacytextblob\", \"sentiment\")\n",
    "\n",
    "if \"emoji\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"emoji\")\n",
    "    \n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions[\"title_polarity\"] = 0.0\n",
    "df_submissions[\"title_subjectivity\"] = 0.0\n",
    "df_submissions[\"body_polarity\"] = 0.0\n",
    "df_submissions[\"body_subjectivity\"] = 0.0\n",
    "df_submissions[\"emojis\"] = \"\"\n",
    "\n",
    "def get_submission_sentiment(title, body):\n",
    "    nlp_title = nlp(str(title))\n",
    "    nlp_body = nlp(str(body))\n",
    "    emojis = nlp_title._.emoji + nlp_body._.emoji\n",
    "    emojis = [emoji[0] for emoji in emojis]\n",
    "    return pd.Series([nlp_title._.polarity, nlp_title._.subjectivity,\n",
    "                      nlp_body._.polarity, nlp_body._.subjectivity, emojis])\n",
    "\n",
    "df_submissions[[\"title_polarity\", \"title_subjectivity\", \"body_polarity\", \"body_subjectivity\", \"emojis\"]] = df_submissions.apply(lambda row: get_submission_sentiment(row[\"title\"], row[\"selftext\"]), axis=1)\n",
    "df_submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by date\n",
    "df_submissions_title_subjectivity = df_submissions.groupby([\"created_utc\"])[\"title_subjectivity\"].sum().reset_index()\n",
    "\n",
    "df_submissions_body_polarity = df_submissions.groupby([\"created_utc\"])[\"body_polarity\"].sum().reset_index()\n",
    "df_submissions_body_subs_title_polarity = df_submissions.groupby([\"created_utc\"])[\"title_polarity\"].sum().reset_index()\n",
    "df_submissionjectivity = df_submissions.groupby([\"created_utc\"])[\"body_subjectivity\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Calculate emojis per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_emojis(data):\n",
    "    return data.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_emojis = df_submissions.groupby([\"created_utc\"]).apply(lambda x: str(list(x[\"emojis\"]))).to_frame(\"emojis\").reset_index()\n",
    "df_submissions_emojis[\"emojis\"] = df_submissions_emojis[\"emojis\"].apply(sanitize_emojis)\n",
    "df_submissions_emojis[\"emojis\"] = df_submissions_emojis.apply(lambda x: dict(Counter(x[\"emojis\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Calculate mentioned stocks and lingos count per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find stocks in a text given stock list\n",
    "regex = re.compile(\"[^a-zA-Z ]\")\n",
    "sp500_set = set(sp500)\n",
    "non_sp500_set = set(non_sp500)\n",
    "lingo_set = set(lingo)\n",
    "\n",
    "def calculate_mentioned_stocks(title, body=\"\"):\n",
    "    content = regex.sub(\"\", str(title) + \" \" + str(body)).split(\" \")\n",
    "    content = set(content)\n",
    "    words_sp500 = str(list(sp500_set & content))\n",
    "    words_non_sp500 = str(list(non_sp500_set & content))\n",
    "    \n",
    "    if len(words_sp500) > 2:\n",
    "        lingos_sp500 = str(list(lingo_set & content))\n",
    "    else:\n",
    "        lingos_sp500 = str([])\n",
    "    \n",
    "    if len(words_non_sp500) > 2:\n",
    "        lingos_non_sp500 = str(list(lingo_set & content))\n",
    "    else:\n",
    "        lingos_non_sp500 = str([])\n",
    "    \n",
    "    return pd.Series([words_sp500, words_non_sp500, lingos_sp500, lingos_non_sp500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions[[\"sp500_stocks\", \"non_sp500_stocks\", \"sp500_lingos\", \"non_sp500_lingos\"]] = df_submissions.apply(lambda x: calculate_mentioned_stocks(x[\"title\"], x[\"selftext\"]), axis=1)\n",
    "df_submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by date\n",
    "df_submissions_sp500_stocks = df_submissions.groupby([\"created_utc\"])[\"sp500_stocks\"].apply(\",\".join).reset_index()\n",
    "df_submissions_non_sp500_stocks = df_submissions.groupby([\"created_utc\"])[\"non_sp500_stocks\"].apply(\",\".join).reset_index()\n",
    "\n",
    "df_submissions_sp500_lingos = df_submissions.groupby([\"created_utc\"])[\"sp500_lingos\"].apply(\",\".join).reset_index()\n",
    "df_submissions_non_sp500_lingos = df_submissions.groupby([\"created_utc\"])[\"non_sp500_lingos\"].apply(\",\".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_column(column):\n",
    "    column = column.replace(\"[],\", \"\").replace(\"[]\", \"\").replace(\"],[\", \",\")\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanitize data\n",
    "df_submissions_sp500_stocks[\"sp500_stocks\"] = df_submissions_sp500_stocks[\"sp500_stocks\"].apply(sanitize_column)\n",
    "df_submissions_non_sp500_stocks[\"non_sp500_stocks\"] = df_submissions_non_sp500_stocks[\"non_sp500_stocks\"].apply(sanitize_column)\n",
    "\n",
    "df_submissions_sp500_lingos[\"sp500_lingos\"] = df_submissions_sp500_lingos[\"sp500_lingos\"].apply(sanitize_column)\n",
    "df_submissions_non_sp500_lingos[\"non_sp500_lingos\"] = df_submissions_non_sp500_lingos[\"non_sp500_lingos\"].apply(sanitize_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count stocks and lingos\n",
    "df_submissions_sp500_stocks[\"sp500_stocks\"] = df_submissions_sp500_stocks[\"sp500_stocks\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_submissions_sp500_stocks[\"sp500_stocks\"] = df_submissions_sp500_stocks.apply(lambda x: dict(Counter(x['sp500_stocks'])), axis=1)\n",
    "\n",
    "df_submissions_non_sp500_stocks[\"non_sp500_stocks\"] = df_submissions_non_sp500_stocks[\"non_sp500_stocks\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_submissions_non_sp500_stocks[\"non_sp500_stocks\"] = df_submissions_non_sp500_stocks.apply(lambda x: dict(Counter(x[\"non_sp500_stocks\"])), axis=1)\n",
    "\n",
    "df_submissions_sp500_lingos[\"sp500_lingos\"] = df_submissions_sp500_lingos[\"sp500_lingos\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_submissions_sp500_lingos[\"sp500_lingos\"] = df_submissions_sp500_lingos.apply(lambda x: dict(Counter(x[\"sp500_lingos\"])), axis=1)\n",
    "\n",
    "df_submissions_non_sp500_lingos[\"non_sp500_lingos\"] = df_submissions_non_sp500_lingos[\"non_sp500_lingos\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_submissions_non_sp500_lingos[\"non_sp500_lingos\"] = df_submissions_non_sp500_lingos.apply(lambda x: dict(Counter(x[\"non_sp500_lingos\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Count per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_count = df_submissions.groupby([\"created_utc\"])[\"title\"].count().reset_index()\n",
    "df_submissions_count.rename(columns={\"title\": \"count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. Calculate total scores per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_score = df_submissions.groupby([\"created_utc\"])[\"score\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6. Calculate total upvote ratio per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_upvote_ratio = df_submissions.groupby([\"created_utc\"])[\"upvote_ratio\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7. Calculate flair category counts per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_category(df):\n",
    "    category_dict = dict(Counter(df[\"category\"]))\n",
    "    if np.nan in category_dict.keys():\n",
    "        category_dict[\"others\"] = category_dict.pop(np.nan)\n",
    "    return category_dict\n",
    "\n",
    "df_submissions_category = df_submissions.groupby([\"created_utc\"]).apply(lambda x: list(x[\"link_flair_text\"])).to_frame(\"category\").reset_index()\n",
    "df_submissions_category[\"category\"] = df_submissions_category.apply(lambda x: sanitize_category(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8. Create the final submissions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_daily = pd.concat([df_submissions_sp500_stocks[\"created_utc\"], \n",
    "                                  df_submissions_title_polarity[\"title_polarity\"], \n",
    "                                  df_submissions_title_subjectivity[\"title_subjectivity\"], \n",
    "                                  df_submissions_body_polarity[\"body_polarity\"], \n",
    "                                  df_submissions_body_subjectivity[\"body_subjectivity\"], \n",
    "                                  df_submissions_count[\"count\"],\n",
    "                                  df_submissions_score[\"score\"],\n",
    "                                  df_submissions_upvote_ratio[\"upvote_ratio\"],\n",
    "                                  df_submissions_emojis[\"emojis\"],\n",
    "                                  df_submissions_category[\"category\"],\n",
    "                                  df_submissions_sp500_stocks[\"sp500_stocks\"], \n",
    "                                  df_submissions_non_sp500_stocks[\"non_sp500_stocks\"],\n",
    "                                  df_submissions_sp500_lingos[\"sp500_lingos\"], \n",
    "                                  df_submissions_non_sp500_lingos[\"non_sp500_lingos\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_submissions_daily.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9. Save the submissions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submissions_daily.to_csv(\"data/feature_submissions.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Calculate sentiment per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_x</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc_x</th>\n",
       "      <th>id_x</th>\n",
       "      <th>score_x</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body_polarity</th>\n",
       "      <th>body_subjectivity</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>I already got in but decided to say YOLO and s...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35t1p</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>Yeah the thing is I actually have some faith i...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl377lu</td>\n",
       "      <td>2</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sf9191</td>\n",
       "      <td>I‚Äôve been with them for two years and have hat...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl389xu</td>\n",
       "      <td>1</td>\n",
       "      <td>l6x2hd</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amcneel</td>\n",
       "      <td>BB</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35j7i</td>\n",
       "      <td>6</td>\n",
       "      <td>l6x2hj</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MineIsLongerThanYour</td>\n",
       "      <td>Not now man. Focus on the fucking gme</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35kwp</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2ho</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_x                                               body  \\\n",
       "0   ADONIS_VON_MEGADONG  I already got in but decided to say YOLO and s...   \n",
       "1   ADONIS_VON_MEGADONG  Yeah the thing is I actually have some faith i...   \n",
       "2                sf9191  I‚Äôve been with them for two years and have hat...   \n",
       "3               amcneel                                                 BB   \n",
       "4  MineIsLongerThanYour              Not now man. Focus on the fucking gme   \n",
       "\n",
       "  created_utc_x     id_x  score_x parent_id  body_polarity  body_subjectivity  \\\n",
       "0    2021-01-28  gl35t1p        3    l6x2gv       0.000000                0.0   \n",
       "1    2021-01-28  gl377lu        2    l6x2gv       0.066667                0.5   \n",
       "2    2021-01-28  gl389xu        1    l6x2hd      -0.900000                0.7   \n",
       "3    2021-01-28  gl35j7i        6    l6x2hj       0.000000                0.0   \n",
       "4    2021-01-28  gl35kwp        3    l6x2ho      -0.600000                0.8   \n",
       "\n",
       "  emojis  \n",
       "0     []  \n",
       "1     []  \n",
       "2     []  \n",
       "3     []  \n",
       "4     []  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[\"body_polarity\"] = 0.0\n",
    "df_comments[\"body_subjectivity\"] = 0.0\n",
    "df_comments[\"emojis\"] = \"\"\n",
    "\n",
    "def get_comment_sentiment(body):\n",
    "    nlp_body = nlp(str(body))\n",
    "    emojis = nlp_body._.emoji\n",
    "    emojis = [emoji[0] for emoji in emojis]\n",
    "    return pd.Series([nlp_body._.polarity, nlp_body._.subjectivity, emojis])\n",
    "\n",
    "df_comments[[\"body_polarity\", \"body_subjectivity\", \"emojis\"]] = df_comments.apply(lambda row: get_comment_sentiment(row[\"body\"]), axis=1)\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date\n",
    "df_comments[\"body_polarity\"] = df_comments[\"body_polarity\"].fillna(0)\n",
    "df_comments[\"body_polarity\"] = df_comments[\"body_polarity\"].astype(float)\n",
    "df_comments_body_polarity = df_comments.groupby([\"created_utc_x\"])[\"body_polarity\"].sum().reset_index()\n",
    "\n",
    "df_comments[\"body_subjectivity\"] = df_comments[\"body_subjectivity\"].fillna(0)\n",
    "df_comments[\"body_subjectivity\"] = df_comments[\"body_subjectivity\"].astype(float)\n",
    "df_comments_body_subjectivity = df_comments.groupby([\"created_utc_x\"])[\"body_subjectivity\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Calculate emojis per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_emojis = df_comments.groupby([\"created_utc_x\"]).apply(lambda x: str(list(x[\"emojis\"]))).to_frame(\"emojis\").reset_index()\n",
    "df_comments_emojis[\"emojis\"] = df_comments_emojis[\"emojis\"].apply(sanitize_emojis)\n",
    "df_comments_emojis[\"emojis\"] = df_comments_emojis.apply(lambda x: dict(Counter(x[\"emojis\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Calculate mentioned stocks and lingos count per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_x</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc_x</th>\n",
       "      <th>id_x</th>\n",
       "      <th>score_x</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body_polarity</th>\n",
       "      <th>body_subjectivity</th>\n",
       "      <th>emojis</th>\n",
       "      <th>sp500_stocks</th>\n",
       "      <th>non_sp500_stocks</th>\n",
       "      <th>sp500_lingos</th>\n",
       "      <th>non_sp500_lingos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>I already got in but decided to say YOLO and s...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35t1p</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADONIS_VON_MEGADONG</td>\n",
       "      <td>Yeah the thing is I actually have some faith i...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl377lu</td>\n",
       "      <td>2</td>\n",
       "      <td>l6x2gv</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sf9191</td>\n",
       "      <td>I‚Äôve been with them for two years and have hat...</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl389xu</td>\n",
       "      <td>1</td>\n",
       "      <td>l6x2hd</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amcneel</td>\n",
       "      <td>BB</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35j7i</td>\n",
       "      <td>6</td>\n",
       "      <td>l6x2hj</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MineIsLongerThanYour</td>\n",
       "      <td>Not now man. Focus on the fucking gme</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>gl35kwp</td>\n",
       "      <td>3</td>\n",
       "      <td>l6x2ho</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_x                                               body  \\\n",
       "0   ADONIS_VON_MEGADONG  I already got in but decided to say YOLO and s...   \n",
       "1   ADONIS_VON_MEGADONG  Yeah the thing is I actually have some faith i...   \n",
       "2                sf9191  I‚Äôve been with them for two years and have hat...   \n",
       "3               amcneel                                                 BB   \n",
       "4  MineIsLongerThanYour              Not now man. Focus on the fucking gme   \n",
       "\n",
       "  created_utc_x     id_x  score_x parent_id  body_polarity  body_subjectivity  \\\n",
       "0    2021-01-28  gl35t1p        3    l6x2gv       0.000000                0.0   \n",
       "1    2021-01-28  gl377lu        2    l6x2gv       0.066667                0.5   \n",
       "2    2021-01-28  gl389xu        1    l6x2hd      -0.900000                0.7   \n",
       "3    2021-01-28  gl35j7i        6    l6x2hj       0.000000                0.0   \n",
       "4    2021-01-28  gl35kwp        3    l6x2ho      -0.600000                0.8   \n",
       "\n",
       "  emojis sp500_stocks non_sp500_stocks sp500_lingos non_sp500_lingos  \n",
       "0     []           []               []           []               []  \n",
       "1     []           []               []           []               []  \n",
       "2     []           []               []           []               []  \n",
       "3     []           []               []           []               []  \n",
       "4     []           []               []           []               []  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[[\"sp500_stocks\", \"non_sp500_stocks\", \"sp500_lingos\", \"non_sp500_lingos\"]] = df_comments.apply(lambda x: calculate_mentioned_stocks(x[\"body\"]), axis=1)\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date\n",
    "df_comments_sp500_stocks = df_comments.groupby([\"created_utc_x\"])[\"sp500_stocks\"].apply(\",\".join).reset_index()\n",
    "df_comments_non_sp500_stocks = df_comments.groupby([\"created_utc_x\"])[\"non_sp500_stocks\"].apply(\",\".join).reset_index()\n",
    "\n",
    "df_comments_sp500_lingos = df_comments.groupby([\"created_utc_x\"])[\"sp500_lingos\"].apply(\",\".join).reset_index()\n",
    "df_comments_non_sp500_lingos = df_comments.groupby([\"created_utc_x\"])[\"non_sp500_lingos\"].apply(\",\".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize data\n",
    "df_comments_sp500_stocks[\"sp500_stocks\"] = df_comments_sp500_stocks[\"sp500_stocks\"].apply(sanitize_column)\n",
    "df_comments_non_sp500_stocks[\"non_sp500_stocks\"] = df_comments_non_sp500_stocks[\"non_sp500_stocks\"].apply(sanitize_column)\n",
    "\n",
    "df_comments_sp500_lingos[\"sp500_lingos\"] = df_comments_sp500_lingos[\"sp500_lingos\"].apply(sanitize_column)\n",
    "df_comments_non_sp500_lingos[\"non_sp500_lingos\"] = df_comments_non_sp500_lingos[\"non_sp500_lingos\"].apply(sanitize_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stocks and lingos\n",
    "df_comments_sp500_stocks[\"sp500_stocks\"] = df_comments_sp500_stocks[\"sp500_stocks\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_comments_sp500_stocks[\"sp500_stocks\"] = df_comments_sp500_stocks.apply(lambda x: dict(Counter(x['sp500_stocks'])), axis=1)\n",
    "\n",
    "df_comments_non_sp500_stocks[\"non_sp500_stocks\"] = df_comments_non_sp500_stocks[\"non_sp500_stocks\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_comments_non_sp500_stocks[\"non_sp500_stocks\"] = df_comments_non_sp500_stocks.apply(lambda x: dict(Counter(x[\"non_sp500_stocks\"])), axis=1)\n",
    "\n",
    "df_comments_sp500_lingos[\"sp500_lingos\"] = df_comments_sp500_lingos[\"sp500_lingos\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_comments_sp500_lingos[\"sp500_lingos\"] = df_comments_sp500_lingos.apply(lambda x: dict(Counter(x[\"sp500_lingos\"])), axis=1)\n",
    "\n",
    "df_comments_non_sp500_lingos[\"non_sp500_lingos\"] = df_comments_non_sp500_lingos[\"non_sp500_lingos\"].str.strip(\"[]\").str.split(\",\")\n",
    "df_comments_non_sp500_lingos[\"non_sp500_lingos\"] = df_comments_non_sp500_lingos.apply(lambda x: dict(Counter(x[\"non_sp500_lingos\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Count  per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_count = df_comments.groupby([\"created_utc_x\"])[\"body\"].count().reset_index()\n",
    "df_comments_count.rename(columns={\"body\": \"count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Calculate total scores per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments[\"score_x\"] = df_comments[\"score_x\"].fillna(0)\n",
    "df_comments[\"score_x\"] = df_comments[\"score_x\"].astype(int)\n",
    "df_comments_score = df_comments.groupby([\"created_utc_x\"])[\"score_x\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Create the final comments dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_daily = pd.concat([df_comments_sp500_stocks[\"created_utc_x\"], \n",
    "                               df_comments_body_polarity[\"body_polarity\"], \n",
    "                               df_comments_body_subjectivity[\"body_subjectivity\"], \n",
    "                               df_comments_count[\"count\"],\n",
    "                               df_comments_score[\"score_x\"],\n",
    "                               df_comments_emojis[\"emojis\"],\n",
    "                               df_comments_sp500_stocks[\"sp500_stocks\"], \n",
    "                               df_comments_non_sp500_stocks[\"non_sp500_stocks\"],\n",
    "                               df_comments_sp500_lingos[\"sp500_lingos\"], \n",
    "                               df_comments_non_sp500_lingos[\"non_sp500_lingos\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc_x</th>\n",
       "      <th>body_polarity</th>\n",
       "      <th>body_subjectivity</th>\n",
       "      <th>count</th>\n",
       "      <th>score_x</th>\n",
       "      <th>emojis</th>\n",
       "      <th>sp500_stocks</th>\n",
       "      <th>non_sp500_stocks</th>\n",
       "      <th>sp500_lingos</th>\n",
       "      <th>non_sp500_lingos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>11215.636037</td>\n",
       "      <td>151064.552591</td>\n",
       "      <td>481439</td>\n",
       "      <td>7307273</td>\n",
       "      <td>{'ü§¨': 130, 'üíé': 44029, 'üöÄ': 129555, 'üòç': 151, ...</td>\n",
       "      <td>{''R'': 184, ''GME'': 38546, ''PM'': 290, ''BB...</td>\n",
       "      <td>{''SNDL'': 742, ''STAY'': 349, ''ON'': 2495, '...</td>\n",
       "      <td>{''BUY'': 2342, ''squeeze'': 937, ''buying'': ...</td>\n",
       "      <td>{''moon'': 146, ''BUY'': 2021, ' 'buying'': 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>24279.126195</td>\n",
       "      <td>234495.826707</td>\n",
       "      <td>714325</td>\n",
       "      <td>11031800</td>\n",
       "      <td>{'üöÄ': 206558, 'ü¶Ö': 25, 'üíé': 107307, '‚úã': 13378...</td>\n",
       "      <td>{''DD'': 1237, ''GME'': 51536, ''CA'': 41, ''A...</td>\n",
       "      <td>{''NEXT'': 1318, ''SNDL'': 306, ' 'EM'': 73, '...</td>\n",
       "      <td>{''DD'': 1275, ''buying'': 3092, ''BUYING'': 5...</td>\n",
       "      <td>{''GO'': 3809, ''go'': 419, ' 'squeeze'': 391,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>21378.698455</td>\n",
       "      <td>139558.128577</td>\n",
       "      <td>400095</td>\n",
       "      <td>5214224</td>\n",
       "      <td>{'ü§∑': 569, 'üèª': 3713, '\\': 3133, 'u': 2911, '2...</td>\n",
       "      <td>{''M'': 525, ''GME'': 34083, ''CCL'': 35, ''SC...</td>\n",
       "      <td>{''AAL'': 119, ''CMPS'': 1, ''UK'': 907, ''EDI...</td>\n",
       "      <td>{''moon'': 1444, ''buying'': 2181, ''DD'': 115...</td>\n",
       "      <td>{''BUY'': 282, ''buying'': 260, ''squeeze'': 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>13794.394890</td>\n",
       "      <td>93609.909582</td>\n",
       "      <td>267732</td>\n",
       "      <td>3151759</td>\n",
       "      <td>{'üíé': 26882, 'üôå': 9807, 'üÖ±': 134, 'Ô∏è': 3030, '...</td>\n",
       "      <td>{''GME'': 25269, ''DO'': 602, ' 'K'': 278, ' '...</td>\n",
       "      <td>{''UK'': 406, ''IDEX'': 9, ''ON'': 491, ''MRVL...</td>\n",
       "      <td>{''holding'': 1067, ''BUY'': 626, ''buying'': ...</td>\n",
       "      <td>{''BUYING'': 90, ''YOLO'': 18, ''GO'': 366, ''...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>12907.351098</td>\n",
       "      <td>109175.041330</td>\n",
       "      <td>335178</td>\n",
       "      <td>4764767</td>\n",
       "      <td>{'üöÄ': 80806, 'üëå': 216, '‚òÄ': 33, 'Ô∏è': 2472, 'üíé'...</td>\n",
       "      <td>{''GME'': 34590, ' 'PM'': 69, ''BBBY'': 122, '...</td>\n",
       "      <td>{''FREE'': 100, ' 'UK'': 28, ''PLUG'': 38, ''L...</td>\n",
       "      <td>{''holding'': 1526, ''DD'': 960, ''BUY'': 1680...</td>\n",
       "      <td>{''go'': 275, ' 'squeeze'': 49, ''BUY'': 699, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc_x  body_polarity  body_subjectivity   count   score_x  \\\n",
       "0    2021-01-28   11215.636037      151064.552591  481439   7307273   \n",
       "1    2021-01-29   24279.126195      234495.826707  714325  11031800   \n",
       "2    2021-01-30   21378.698455      139558.128577  400095   5214224   \n",
       "3    2021-01-31   13794.394890       93609.909582  267732   3151759   \n",
       "4    2021-02-01   12907.351098      109175.041330  335178   4764767   \n",
       "\n",
       "                                              emojis  \\\n",
       "0  {'ü§¨': 130, 'üíé': 44029, 'üöÄ': 129555, 'üòç': 151, ...   \n",
       "1  {'üöÄ': 206558, 'ü¶Ö': 25, 'üíé': 107307, '‚úã': 13378...   \n",
       "2  {'ü§∑': 569, 'üèª': 3713, '\\': 3133, 'u': 2911, '2...   \n",
       "3  {'üíé': 26882, 'üôå': 9807, 'üÖ±': 134, 'Ô∏è': 3030, '...   \n",
       "4  {'üöÄ': 80806, 'üëå': 216, '‚òÄ': 33, 'Ô∏è': 2472, 'üíé'...   \n",
       "\n",
       "                                        sp500_stocks  \\\n",
       "0  {''R'': 184, ''GME'': 38546, ''PM'': 290, ''BB...   \n",
       "1  {''DD'': 1237, ''GME'': 51536, ''CA'': 41, ''A...   \n",
       "2  {''M'': 525, ''GME'': 34083, ''CCL'': 35, ''SC...   \n",
       "3  {''GME'': 25269, ''DO'': 602, ' 'K'': 278, ' '...   \n",
       "4  {''GME'': 34590, ' 'PM'': 69, ''BBBY'': 122, '...   \n",
       "\n",
       "                                    non_sp500_stocks  \\\n",
       "0  {''SNDL'': 742, ''STAY'': 349, ''ON'': 2495, '...   \n",
       "1  {''NEXT'': 1318, ''SNDL'': 306, ' 'EM'': 73, '...   \n",
       "2  {''AAL'': 119, ''CMPS'': 1, ''UK'': 907, ''EDI...   \n",
       "3  {''UK'': 406, ''IDEX'': 9, ''ON'': 491, ''MRVL...   \n",
       "4  {''FREE'': 100, ' 'UK'': 28, ''PLUG'': 38, ''L...   \n",
       "\n",
       "                                        sp500_lingos  \\\n",
       "0  {''BUY'': 2342, ''squeeze'': 937, ''buying'': ...   \n",
       "1  {''DD'': 1275, ''buying'': 3092, ''BUYING'': 5...   \n",
       "2  {''moon'': 1444, ''buying'': 2181, ''DD'': 115...   \n",
       "3  {''holding'': 1067, ''BUY'': 626, ''buying'': ...   \n",
       "4  {''holding'': 1526, ''DD'': 960, ''BUY'': 1680...   \n",
       "\n",
       "                                    non_sp500_lingos  \n",
       "0  {''moon'': 146, ''BUY'': 2021, ' 'buying'': 68...  \n",
       "1  {''GO'': 3809, ''go'': 419, ' 'squeeze'': 391,...  \n",
       "2  {''BUY'': 282, ''buying'': 260, ''squeeze'': 2...  \n",
       "3  {''BUYING'': 90, ''YOLO'': 18, ''GO'': 366, ''...  \n",
       "4  {''go'': 275, ' 'squeeze'': 49, ''BUY'': 699, ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6. Save the comments dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_daily.to_csv(feature_comments_csv, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments.drop([4929153, 4929185, 4929199, 4929235, 4929244, 5015671, 5015833, 5016105, 5021356], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
